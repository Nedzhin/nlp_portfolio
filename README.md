# nlp_portfolio

Replication of the ["Evaluating Parameter-Efficient Finetuning Approaches for Pre-trained Models on the Financial Domain"](https://aclanthology.org/2023.findings-emnlp.1035/)

# Peft methods
Recent findings show that LoRa and Adapter fine-tuning methods show the similar performance as the full fine-tuning while saving time, computational resources and memory.

# Steps done
We repeated the experiment and got nearly the same performance and confirm that paper's results are reliable

## The tasks in Finance tested here are
- Question Answering
- Named Entity Recognition
- News Headline Classification
- Sentiment analysis

# Link to the Notebook: is here -> [Notebook](https://colab.research.google.com/drive/1Kjzc9WSghctUIXwAOh0g8dLF-UuYrBgh?usp=sha ring)